{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUrXDl0PAmNUejQ4dvlZAP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jegovila/SI4/blob/main/5%20Clasificaci%C3%B3n%20n%C3%BAmeros%20y%20PCA/C%C3%B3digos/python/PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción"
      ],
      "metadata": {
        "id": "UOYJ777PDjrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA es uno de los algoritmos no supervisados màs utilizados. Es un algoritmo de reducción de dimensionalidad, pero también puede ser útil como herramienta para visualización, filtrado de ruido, etc."
      ],
      "metadata": {
        "id": "Fv52tohLEK_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')"
      ],
      "metadata": {
        "id": "gnYRnoiYDnZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.RandomState(1)\n",
        "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal');"
      ],
      "metadata": {
        "id": "wuYuYq9CDxto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple vista, está claro que existe una relación casi lineal entre las variables $x$ y $y$. Esto recuerda a la Regresión lineal, pero el planteamiento del problema aquí es diferente: en lugar de intentar predecir los valores de $y$ a partir de los valores de $x$, el problema de aprendizaje no supervisado intenta aprender sobre la relación entre los valores de $x$ y $y$.\n",
        "\n",
        "En el análisis de componentes principales, esta relación se cuantifica encontrando una lista de los ejes principales en los datos y utilizando esos ejes para describir el conjunto de datos. Usando el estimador PCA de Scikit-Learn, podemos calcular esto de la siguiente manera:\n"
      ],
      "metadata": {
        "id": "g1yectDQEtHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)"
      ],
      "metadata": {
        "id": "kLbNfBHZDzwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo aprende algunas cantidades de los datos, sobre todo los componentes y la varianza explicada:"
      ],
      "metadata": {
        "id": "ihZ9ApGDFYG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca.components_)"
      ],
      "metadata": {
        "id": "DiNkaf53D2xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca.explained_variance_)"
      ],
      "metadata": {
        "id": "NihMk4bUD5BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ver qué significan estos números, visualicémoslos como vectores sobre los datos de entrada, usando los componentes para definir la dirección del vector y la varianza explicada para definir la longitud al cuadrado del vector:"
      ],
      "metadata": {
        "id": "JCgjhCw9FeI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops=dict(arrowstyle='->', linewidth=2,\n",
        "                    shrinkA=0, shrinkB=0)\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    draw_vector(pca.mean_, pca.mean_ + v)\n",
        "plt.axis('equal');"
      ],
      "metadata": {
        "id": "d7ywNuRiD7Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estos vectores representan los ejes principales de los datos, y la longitud de cada vector es una indicación de cuán \"importante\" es ese eje para describir la distribución de los datos; más precisamente, es una medida de la varianza de los datos cuando se proyectan sobre ese eje. La proyección de cada punto de datos sobre los ejes principales son los componentes principales de los datos.\n",
        "\n",
        "\n",
        "Esta transformación de ejes de datos a ejes principales es una transformación afín, lo que significa que se compone de traslación, rotación y escalado uniforme.\n"
      ],
      "metadata": {
        "id": "fPXFM_L1Ft2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA como reducción de la dimensionalidad"
      ],
      "metadata": {
        "id": "frWrKTnfGC4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El uso de PCA para la reducción de dimensionalidad implica poner a cero uno o más de los componentes principales más pequeños, lo que da como resultado una proyección de los datos de menor dimensión que preserva la varianza máxima de los datos."
      ],
      "metadata": {
        "id": "m7R2xEfhGFvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "print(\"original shape:   \", X.shape)\n",
        "print(\"transformed shape:\", X_pca.shape)"
      ],
      "metadata": {
        "id": "dTd52DGUECAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los datos transformados se han reducido a una única dimensión. Para comprender el efecto de esta reducción de dimensionalidad, podemos realizar la transformación inversa de estos datos reducidos y trazarlos junto con los datos originales:"
      ],
      "metadata": {
        "id": "gRHMh7TsGQ1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = pca.inverse_transform(X_pca)\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
        "plt.axis('equal');"
      ],
      "metadata": {
        "id": "8Hm7f9m3ED2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este conjunto de datos de dimensión reducida es, en cierto sentido, \"lo suficientemente bueno\" para codificar las relaciones más importantes entre los puntos: a pesar de reducir el número de características de datos en un 50%, las relaciones generales entre los puntos de datos se conservan en su mayoría."
      ],
      "metadata": {
        "id": "oDCyJau-GUYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplos"
      ],
      "metadata": {
        "id": "Mv43YL_ODlpe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzKxq2edpwkP"
      },
      "outputs": [],
      "source": [
        "!wget -nc \"https://raw.githubusercontent.com/Jegovila/SI4/main/5%20Clasificaci%C3%B3n%20n%C3%BAmeros%20y%20PCA/C%C3%B3digos/python/wine.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "nNvYoXXCrECI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"wine.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "WE7VlXq2rKta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:,1:].describe()"
      ],
      "metadata": {
        "id": "7tr2xMujreu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in df.columns[1:]:\n",
        "    df.boxplot(c,by='Class',figsize=(7,4),fontsize=14)\n",
        "    plt.title(\"{}\\n\".format(c),fontsize=16)\n",
        "    plt.xlabel(\"Wine Class\", fontsize=16)"
      ],
      "metadata": {
        "id": "SaL95cWBtTH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.figure(figsize=(10,6))\n",
        "plt.scatter(df['OD280/OD315 of diluted wines'],df['Flavanoids'],c=df['Class'],edgecolors='k',alpha=0.75,s=150)\n",
        "plt.grid(True)\n",
        "plt.title(\"Scatter plot of two features showing the \\ncorrelation and class seperation\",fontsize=15)\n",
        "plt.xlabel(\"OD280/OD315 of diluted wines\",fontsize=15)\n",
        "plt.ylabel(\"Flavanoids\",fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "clqXVSkewB1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_matrix(df):\n",
        "    from matplotlib import pyplot as plt\n",
        "    from matplotlib import cm as cm\n",
        "\n",
        "    fig = plt.figure(figsize=(8,6))\n",
        "    ax1 = fig.add_subplot(111)\n",
        "    cmap = cm.get_cmap('jet', 30)\n",
        "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
        "    ax1.grid(True)\n",
        "    plt.title('Wine data set features correlation\\n',fontsize=15)\n",
        "    labels=df.columns\n",
        "    ax1.set_xticklabels(labels,fontsize=9)\n",
        "    ax1.set_yticklabels(labels,fontsize=9)\n",
        "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
        "    fig.colorbar(cax, ticks=[0.1*i for i in range(-11,11)])\n",
        "    plt.show()\n",
        "\n",
        "correlation_matrix(df)"
      ],
      "metadata": {
        "id": "nGUVeBoazclA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "vBIIMx0cz5BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "k10ElyM_z6Qc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "Y4m-Q66xz8UY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Class',axis=1)\n",
        "y = df['Class']"
      ],
      "metadata": {
        "id": "HQPT3RwBz96k"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "4fJR8_lB0BnL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfx = pd.DataFrame(data=X,columns=df.columns[1:])\n",
        "dfx"
      ],
      "metadata": {
        "id": "t0qEdcNB0hWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import PCA"
      ],
      "metadata": {
        "id": "XqI26TR71BTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "vXgxt8HI1AH0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=None)\n",
        "pca.fit(dfx)"
      ],
      "metadata": {
        "id": "qJUIiyDB1E4J"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "nb5zwNWy1dOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(pca.explained_variance_ratio_.cumsum(), marker = \"o\", linestyle=\"--\")\n",
        "plt.title(\"Explained variance by Components\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Sum\")"
      ],
      "metadata": {
        "id": "dBgAq41k1-gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfx_trans = pca.transform(dfx)\n",
        "dfx_trans = pd.DataFrame(data=dfx_trans)\n",
        "dfx_trans.head(10)"
      ],
      "metadata": {
        "id": "GgG6WtQa5WjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(dfx_trans[0],dfx_trans[1],c=df['Class'],edgecolors='k',alpha=0.75,s=150)\n",
        "plt.grid(True)\n",
        "plt.title(\"Class separation using first two principal components\\n\",fontsize=20)\n",
        "plt.xlabel(\"Principal component-1\",fontsize=15)\n",
        "plt.ylabel(\"Principal component-2\",fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5VD8lC7B5QdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}